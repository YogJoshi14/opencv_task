{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_infer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZNtI+C9e0OrSVtZVZBIYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YogJoshi14/opencv_task/blob/main/CV_infer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSVIU-OWw5Yz"
      },
      "source": [
        "Model link:\n",
        "\n",
        "[fastcnn with resnet50 backbone](https://drive.google.com/file/d/1ExpHnZaDPCLXrX00QNkYavxjAlcSeK09/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L37aVHcRuheH"
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "def get_model(num_classes):\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CLASSES = ['laptop','lights']\n",
        "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
        "\n",
        "# load the image from disk\n",
        "\n",
        "model_path = '/content/drive/MyDrive/fcnnresnet50.pth' #path to model\n",
        "model = get_model(len(CLASSES)+1)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "else:\n",
        "    model.cpu()\n",
        "\n",
        "\n",
        "def detection_on_video(model,input,output):\n",
        "  writer = None\n",
        "\n",
        "  # initialize the frame dimensions (we'll set them as soon as we read\n",
        "  # the first frame from the video)\n",
        "  W = None\n",
        "  H = None\n",
        "  vs = cv2.VideoCapture(input)\n",
        "  while True:\n",
        "    # grab the next frame and handle if we are reading from either\n",
        "    # VideoCapture or VideoStream\n",
        "    _,frame = vs.read()\n",
        "    if input is not None and frame is None:\n",
        "        break\n",
        "    orig = frame.copy()\n",
        "    # convert the image from BGR to RGB channel ordering and change the\n",
        "    # image from channels last to channels first ordering\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    image = image.transpose((2, 0, 1))\n",
        "    # add the batch dimension, scale the raw pixel intensities to the\n",
        "    # range [0, 1], and convert the image to a floating point tensor\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    image = torch.FloatTensor(image)\n",
        "    # send the input to the device and pass the it through the network to\n",
        "    # get the detections and predictions\n",
        "    if torch.cuda.is_available():\n",
        "      image = image.to(DEVICE)\n",
        "    detections = model(image)[0]\n",
        "    # loop over the detections\n",
        "    for i in range(0, len(detections[\"boxes\"])):\n",
        "      # extract the confidence (i.e., probability) associated with the\n",
        "      # prediction\n",
        "      confidence = detections[\"scores\"][i]\n",
        "      # filter out weak detections by ensuring the confidence is\n",
        "      # greater than the minimum confidence\n",
        "      if confidence > 0.6:\n",
        "          # extract the index of the class label from the detections,\n",
        "          # then compute the (x, y)-coordinates of the bounding box\n",
        "          # for the object\n",
        "          idx = int(detections[\"labels\"][i])-1\n",
        "          box = detections[\"boxes\"][i].detach().cpu().numpy()\n",
        "          (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "          # display the prediction to our terminal\n",
        "          label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
        "          print(\"[INFO] {}\".format(label))\n",
        "          # draw the bounding box and label on the image\n",
        "          cv2.rectangle(orig, (startX, startY), (endX, endY),\n",
        "              COLORS[idx], 2)\n",
        "          y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "          cv2.putText(orig, label, (startX, y),\n",
        "              cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
        "    # show the output image\n",
        "\n",
        "    # if the frame dimensions are empty, set them\n",
        "    if W is None or H is None:\n",
        "      (H, W) = frame.shape[:2]\n",
        "\n",
        "    # if we are supposed to be writing a video to disk, initialize\n",
        "    # the writer\n",
        "    if input is not None and writer is None:\n",
        "      fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "      writer = cv2.VideoWriter(output, fourcc, 30,\n",
        "        (W, H), True)\n",
        "    if writer is not None:\n",
        "      writer.write(orig)\n",
        "\n",
        "\t# # show the output frame\n",
        "  #   cv2.imshow(\"Frame\", frame)\n",
        "  #   key = cv2.waitKey(1) & 0xFF\n",
        "  # plt.imshow(orig)\n",
        "  #   # if the `q` key was pressed, break from the loop\n",
        "  #   if key == ord(\"q\"):\n",
        "  #     break\n",
        "  if writer is not None:\n",
        "    writer.release()\n",
        "  # vs.release()\n",
        "\n",
        "  # close any open windows\n",
        "  # cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3fcJNBhu2X1"
      },
      "source": [
        "input = '/content/drive/MyDrive/lap.mp4' #input video\n",
        "output = '/content/drive/MyDrive/output_lap.mp4' #output video\n",
        "detection_on_video(model,input,output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}